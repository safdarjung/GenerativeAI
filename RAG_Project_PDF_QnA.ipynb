{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install tensorflow\n",
    "# %pip install torch\n",
    "#pip install pydantic==1.10.8\n",
    "#pip install pydantic\n",
    "# %pip install \"pinecone-client[grpc]\"\n",
    "# %pip install langchain_pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['PINECONE_API_KEY'] = 'ff509895-3d6b-423b-ba58-88df2d70ac2b'\n",
    "pinecone_api_key = os.environ['PINECONE_API_KEY']\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# MODEL = 'gpt-3.5-turbo'\n",
    "MODEL = 'llama3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you laugh!\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "if MODEL.startswith('gpt'):\n",
    "    model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=MODEL )\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "    \n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "\n",
    "# model.invoke('Tell me a joke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As the clock struck midnight, Emma crept into the abandoned mansion to explore. She had always been drawn to the whispers of hidden treasures within its walls. Suddenly, she stumbled upon a dusty old key and a faint light flickered to life in front of her. The room began to glow with an otherworldly radiance, revealing a secret garden hidden for decades. In the center of the garden, Emma found a beautiful music box playing her favorite lullaby.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = model | parser\n",
    "chain.invoke('tell a 5 line story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"1706.03762v7.pdf\")\n",
    "pdf_text = loader.load()\n",
    "pdf_text_list = [doc.page_content for doc in pdf_text]  \n",
    "\n",
    "\n",
    "# pages= loader.load_and_split()\n",
    "# pages\n",
    "doc = \"\\n\".join(pdf_text_list)  # Join all pages into a single string\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"##\", \"Header 2\")\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAnswer the questions based on the context from the given data. If you don't know the answer, just say that you don't know.\\n\\n\\ncontext: Here is some context\\nquestion: here is a question\\n\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = '''\n",
    "Answer the questions based on the context from the given data. If you don't know the answer, just say that you don't know.\n",
    "\n",
    "\n",
    "context: {context}\n",
    "question: {question}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(template=template)\n",
    "prompt.format(context = 'Here is some context', question='here is a question')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the context that the temperature has been rising over the past 10 years, I would answer that I don't know what the temperature will be next year. The trend in the data suggests an upward trajectory, but there is no specific information provided to predict exactly what the temperature will be next year.\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": 'the data from the previous 10 years show that the temperature is on a rise',\n",
    "        \"question\": 'temperature this year is 20 degrees. What will be the temperature next year?'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'context': {'title': 'Context', 'type': 'string'},\n",
       "  'question': {'title': 'Question', 'type': 'string'}},\n",
       " 'required': ['context', 'question']}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key='ff509895-3d6b-423b-ba58-88df2d70ac2b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"docs-quickstart-index\"\n",
    "embedding_dim = 1024\n",
    "\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(name=index_name)\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=embedding_dim,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud='aws', \n",
    "        region='us-east-1'\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "# # used for making small vectorstores, for large ones use Pinecone\n",
    "\n",
    "# vectorstore = DocArrayInMemorySearch.from_documents(pages,\n",
    "#                                                     embedding=embeddings,)\n",
    "from langchain_pinecone import PineconeEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=embedding_dim,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    ) \n",
    "\n",
    "model_name = \"multilingual-e5-large\"  \n",
    "embeddings = PineconeEmbeddings(  \n",
    "    model=model_name,  \n",
    "    pinecone_api_key=pinecone_api_key\n",
    ")  \n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=md_header_splits,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings, \n",
    "    namespace=\"wondervector5000\" \n",
    ")\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever retrirves information from anywhere, in this case from the vectorstore. It can be any other object\n",
    "\n",
    "# documents = vectorstore.as_retriever().invoke(\"MultiHead Attention\")\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "namespace = \"wondervector5000\"\n",
    "\n",
    "for ids in index.list(namespace=namespace):\n",
    "    query = index.query(\n",
    "        id=ids[0], \n",
    "        namespace=namespace, \n",
    "        top_k=1,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001ED09CFA710>\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA  \n",
    "\n",
    "\n",
    "knowledge = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    namespace=\"wondervector5000\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=knowledge.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "\n",
      "Chat with knowledge:\n",
      "Based on the provided context, here are three important facts about Transformers:\n",
      "\n",
      "1. **Attention Mechanism**: Transformers use an attention mechanism that allows them to focus on specific parts of the input sequence (in this case, sentences) and weigh their importance. This is demonstrated in Figures 3-5, which show how different attention heads attend to distant dependencies or perform tasks like anaphora resolution.\n",
      "2. **Encoder Self-Attention**: The attention mechanism is used in the encoder self-attention layer, which allows the model to consider long-range dependencies within a sentence. This is shown in Figure 3, where multiple attention heads attend to a distant dependency of the verb \"making\".\n",
      "3. **Multi-Head Attention**: Transformers use multi-head attention, which allows different attention heads to perform different tasks and weigh their importance. This is demonstrated in Figures 4 and 5, which show two attention heads involved in anaphora resolution and another head exhibiting behavior related to sentence structure.\n",
      "\n",
      "These facts highlight the key innovations that make Transformers effective for machine translation and natural language processing tasks.\n",
      "\n",
      "Chat without knowledge:\n",
      "Here are three important facts about Transformers:\n",
      "\n",
      "1. **The Transformers were originally a toy franchise**: The Transformers were created by Hasbro and Takara Tomy as a line of transforming toys in the 1980s. The initial series, called Generation 1 (G1), was launched in 1984 and consisted of a range of robots that could change into various forms, such as cars, trucks, planes, and animals.\n",
      "\n",
      "2. **The Transformers are based on sentient machines**: In the Transformers franchise, the robots can change their physical form to mimic other objects or animals, but they are also capable of intelligent thought and emotional expression. The Autobots (led by Optimus Prime) are the heroic faction that fights against the evil Decepticons.\n",
      "\n",
      "3. **The Transformers have a rich continuity**: While there have been various animated series, comic book stories, and movies featuring the Transformers over the years, the franchise has a complex continuity that can be confusing for new fans. The original G1 series was followed by several sequels, including Generation 2 (G2), Beast Wars, Armada, Energon, Cybertron, Animated, Prime, and Robots in Disguise. Each series has its own unique storylines and characters, but they all take place within the larger Transformers universe.\n",
      "\n",
      "Query 2\n",
      "\n",
      "Chat with knowledge:\n",
      "The Multihead Attention mechanism!\n",
      "\n",
      "In this context, we're dealing with Neural Machine Translation (NMT) systems, specifically the attention visualizations from a paper by Qin Gao et al. [1] and some examples of attention heads in action.\n",
      "\n",
      "**What is Multihead Attention?**\n",
      "\n",
      "Multihead Attention is an extension of the traditional Self-Attention mechanism, popularized in the Transformer architecture [2]. The original Self-Attention mechanism allows the model to attend to all input elements simultaneously and weigh their importance using a learned weighting function (the \"attention weights\"). This helps the model understand relationships between different parts of the input sequence.\n",
      "\n",
      "**How does Multihead Attention work?**\n",
      "\n",
      "In Multihead Attention, instead of having a single attention mechanism, we have multiple parallel attention mechanisms, each with its own set of learnable weights and biases. These are called \"heads\" or \"attention heads.\" Each head is applied to the same input sequence, but with different learned weights and biases.\n",
      "\n",
      "During training, each head computes an attention weight vector for each input element, just like in traditional Self-Attention. However, these vectors are then concatenated and transformed using a linear layer followed by a softmax function, resulting in a single attention score vector per input element.\n",
      "\n",
      "**Why do we need Multihead Attention?**\n",
      "\n",
      "By having multiple heads, the model can capture different aspects of the input sequence simultaneously. This is particularly useful for NMT tasks, where capturing long-distance dependencies and complex relationships between tokens is crucial.\n",
      "\n",
      "In Figure 3, for example, we see that many attention heads attend to distant dependencies in the sentence, completing phrases like \"making...more difficult.\" In Figure 4, two attention heads seem to be involved in anaphora resolution (resolving pronouns).\n",
      "\n",
      "**Key takeaways:**\n",
      "\n",
      "1. Multihead Attention is an extension of Self-Attention, with multiple parallel attention mechanisms (heads) applied simultaneously.\n",
      "2. Each head learns its own set of weights and biases, allowing it to capture different aspects of the input sequence.\n",
      "3. The output of each head is concatenated and transformed using a linear layer followed by softmax, resulting in a single attention score vector per input element.\n",
      "\n",
      "Now you know how Multihead Attention works!\n",
      "\n",
      "Chat without knowledge:\n",
      "Multi-head attention is a crucial component in transformer models, introduced by Vaswani et al. in their 2017 paper \"Attention Is All You Need\". It's a way to attend to different parts of the input sequence simultaneously and gather information from multiple sources.\n",
      "\n",
      "**Why do we need multi-head attention?**\n",
      "\n",
      "In traditional attention mechanisms, such as dot-product attention or scaled dot-product attention, the model attends to all parts of the input sequence sequentially. However, this can be inefficient for several reasons:\n",
      "\n",
      "1. **Sequential processing**: Processing one part of the input at a time can lead to slow convergence and decreased performance.\n",
      "2. ** Limited capacity**: Attending to only one part of the input sequence at a time limits the model's ability to capture complex relationships between different parts.\n",
      "\n",
      "**How does multi-head attention work?**\n",
      "\n",
      "To overcome these limitations, Vaswani et al. introduced the concept of multi-head attention, which involves applying multiple attention mechanisms in parallel, each with its own set of learnable weights and biases. This allows the model to attend to different parts of the input sequence simultaneously and gather information from multiple sources.\n",
      "\n",
      "Here's a step-by-step explanation:\n",
      "\n",
      "1. **Linear projections**: The input sequence is passed through multiple linear projections (usually 8-16) to generate Q, K, and V matrices for each attention head.\n",
      "2. **Attention heads**: Each attention head applies scaled dot-product attention independently. This means that each head attends to different parts of the input sequence simultaneously.\n",
      "3. **Concatenation**: The outputs from all attention heads are concatenated to form a single output vector.\n",
      "4. **Linear transformation**: Finally, a linear transformation is applied to the concatenated output to produce the final output.\n",
      "\n",
      "**Key benefits**\n",
      "\n",
      "Multi-head attention offers several advantages:\n",
      "\n",
      "1. **Improved parallelization**: By applying multiple attention mechanisms in parallel, multi-head attention enables more efficient processing and faster training times.\n",
      "2. **Increased expressiveness**: The combination of multiple attention heads allows the model to capture complex relationships between different parts of the input sequence.\n",
      "3. **Robustness to noise**: Multi-head attention can help the model be more robust to noisy or corrupted input sequences by averaging out errors across multiple attention heads.\n",
      "\n",
      "**Practical implementation**\n",
      "\n",
      "In a transformer model, multi-head attention is typically implemented using the following layers:\n",
      "\n",
      "1. Self-attention layer (SA): This layer applies multi-head attention to the input sequence and generates an output vector.\n",
      "2. Feed-forward network (FFN) layer: This layer applies a non-linear transformation to the output from the SA layer.\n",
      "\n",
      "By combining self-attention, feed-forward networks, and multi-head attention, transformer models have achieved state-of-the-art results in various natural language processing tasks, including machine translation, text classification, and question answering.\n"
     ]
    }
   ],
   "source": [
    "# cleaned_docs = []\n",
    "# for doc in documents:\n",
    "#     content = doc.page_content\n",
    "#     cleaned_content = ' '.join(content.split())\n",
    "#     cleaned_docs.append(cleaned_content)\n",
    "# with open('cleaned_output.txt', 'w') as f:\n",
    "#     for doc in cleaned_docs:\n",
    "#         f.write(doc + '\\n\\n')  \n",
    "\n",
    "\n",
    "# Define a few questions about the WonderVector5000.\n",
    "query1 = \"\"\"What are the first 3 important facts about transformers\"\"\"\n",
    "\n",
    "query2 = \"\"\"explain Multihead attention\"\"\"\n",
    "\n",
    "# Send each query to the LLM twice, first with relevant knowledge from Pincone \n",
    "# and then without any additional knowledge.\n",
    "print(\"Query 1\\n\")\n",
    "print(\"Chat with knowledge:\")\n",
    "print(qa.invoke(query1).get(\"result\"))\n",
    "print(\"\\nChat without knowledge:\")\n",
    "print(model.invoke(query1))\n",
    "print(\"\\nQuery 2\\n\")\n",
    "print(\"Chat with knowledge:\")\n",
    "print(qa.invoke(query2).get(\"result\"))\n",
    "print(\"\\nChat without knowledge:\")\n",
    "print(model.invoke(query2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
